# References

The Viet Bui, Thi Oanh Tran, and Phuong Le-Hong. 2020. Improving Sequence Tagging for Vietnamese Text using Transformer-based Neural Models. In Proceedings of PACLIC, pages 13–20.

Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: Pre-training text encoders as discriminators rather than
generators. In Proceedings of ICLR.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
Cross-lingual Representation Learning at Scale. In
Proceedings of ACL, pages 8440–8451.

Mai Hoang Dao, Thinh Hung Truong, and Dat Quoc
Nguyen. 2021. Intent Detection and Slot Filling for
Vietnamese. In Proceedings of InterSpeech, pages
4698–4702.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of NAACL, pages 4171–
4186.

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. 2019. Unified Language
Model Pre-training for Natural Language Under-
standing and Generation.
In Proceedings of NeurIPS, volume 32.

Moussa Kamal Eddine, Antoine J-P Tixier, and
Michalis Vazirgiannis. 2020. BARThez: a Skilled
Pretrained French Sequence-to-Sequence Model.
arXiv preprint, arXiv:2010.12321.

Dan Hendrycks and Kevin Gimpel. 2016. Gaus-
sian Error Linear Units (GELUs). arXiv preprint,
arXiv:1606.08415.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In Proceedings
of ICLR.

Taku Kudo and John Richardson. 2018. Sentence-
Piece: A simple and language independent subword
tokenizer and detokenizer for Neural Text Process-
ing. In Proceedings of EMNLP: System Demonstra-
tions, pages 66–71.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising Sequence-to-Sequence
Pre-training for Natural Language Generation,
Translation, and Comprehension. In Proceedings of
ACL, pages 7871–7880.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summariza-
tion Branches Out, pages 74–81.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual Denoising
Pre-training for Neural Machine Translation. Trans-
actions of the ACL, 8:726–742.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A Robustly Optimized BERT Pretrain-
ing Approach. arXiv preprint, arXiv:1907.11692.

Dat Quoc Nguyen and Anh Tuan Nguyen. 2020.
PhoBERT: Pre-trained language models for Viet-
namese. In Findings of EMNLP, pages 1037–1042.

Hieu Nguyen, Long Phan, James Anibal, Alec Pel-
tekian, and Hieu Tran. 2021. VieSum: How Robust
Are Transformer-based Models on Vietnamese Sum-
marization? arXiv preprint, arXiv:2110.04257v1.

Linh The Nguyen and Dat Quoc Nguyen. 2021.
PhoNLP: A joint multi-task learning model for Viet-
namese part-of-speech tagging, named entity recog-
nition and dependency parsing. In Proceedings of
NAACL: Demonstrations, pages 1–7.

Van-Hau Nguyen, Thanh-Chinh Nguyen, Minh-Tien
Nguyen, and Nguyen Xuan Hoai. 2019. VNDS: A
Vietnamese Dataset for Summarization. In Proceed-
ings of NICS, pages 375–380.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A Fast, Extensible
Toolkit for Sequence Modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, pages 48–53.

Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun
Yao, Bartuer Zhou, Biao Cheng, Daxin Jiang,
Jiusheng Chen, Ruofei Zhang, Houqiang Li, and
Nan Duan. 2021. ProphetNet-X: Large-Scale Pre-
training Models for English, Chinese, Multi-lingual,
Dialog, and Code Generation. In Proceedings of
ACL: System Demonstrations, pages 232–239.

Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu,
Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming
Zhou. 2020. ProphetNet: Predicting Future N-gram
for Sequence-to-SequencePre-training. In Findings
of EMNLP, pages 2401–2410.

Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the Limits of Transfer Learning with a Unified Text-
to-Text Transformer. Journal of Machine Learning
Research, 21(140):1–67.

Sascha Rothe, Shashi Narayan, and Aliaksei Severyn.
2020. Leveraging Pre-trained Checkpoints for Se-
quence Generation Tasks. Transactions of the ACL,
8:264–280.

Sebastian Ruder. 2020. Why You Should Do NLP Be-
yond English. https://ruder.io/nlp-beyond-english/.



Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of ACL, pages 1715–1725.

Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai,
Fei Yang, Li Zhe, Hujun Bao, and Xipeng Qiu.
2021. CPT: A Pre-Trained Unbalanced Transformer
for Both Chinese Language Understanding and Gen-
eration. arXiv preprint, arXiv:2109.05729.

Dinh Quang Thang, Le Hong Phuong, Nguyen
Thi Minh Huyen, Nguyen Cam Tu, Mathias Rossig-
nol, and Vu Xuan Luong. 2008. Word segmentation
of Vietnamese texts: a comparison of approaches.
In Proceedings of LREC, pages 1933–1936.

Dang Van Thin, Lac Si Le, Vu Xuan Hoang, and
Ngan Luu-Thuy Nguyen. 2021. Investigating Mono-
lingual and Multilingual BERT Models for Viet-
namese Aspect Category Detection. arXiv preprint,
arXiv:2103.09519.

Thinh Hung Truong, Mai Hoang Dao, and Dat Quoc
Nguyen. 2021. COVID-19 Named Entity Recog-
nition for Vietnamese. In Proceedings of NAACL-
HLT, pages 2146–2153.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Proceedings of NIPS, volume 30.

Alex Wang and Kyunghyun Cho. 2019. BERT has
a Mouth, and It Must Speak: BERT as a Markov
Random Field Language Model. arXiv preprint,
arXiv:1902.04094.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-Art Natural Language Pro-
cessing. In Proceedings of EMNLP 2020: System
Demonstrations, pages 38–45.

Linting Xue, Aditya Barua, Noah Constant, Rami Al-
Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
and Colin Raffel. 2021a. ByT5: Towards a token-
free future with pre-trained byte-to-byte models.
arXiv preprint, arXiv:2105.13626.

Linting Xue, Noah Constant, Adam Roberts, Mihir
Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,
and Colin Raffel. 2021b. mT5: A Massively Mul-
tilingual Pre-trained Text-to-Text Transformer. In
Proceedings of NAACL, pages 483–498.

Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-
ter Liu. 2020. PEGASUS: Pre-training with ex-
tracted gap-sentences for abstractive summarization.
In Proceedings of ICML, pages 11328–11339.