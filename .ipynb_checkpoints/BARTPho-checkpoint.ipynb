{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34f93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b664ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d49cb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module transformers.models.bartpho has no attribute BartphoTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#BARTpho-syllable\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m syllable_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:522\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     tokenizer_class_candidate \u001b[38;5;241m=\u001b[39m config_tokenizer_class\n\u001b[0;32m--> 522\u001b[0m     tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_class_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_class_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:253\u001b[0m, in \u001b[0;36mtokenizer_class_from_name\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m    250\u001b[0m         module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(module_name)\n\u001b[1;32m    252\u001b[0m         module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 253\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config, tokenizers \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tokenizer \u001b[38;5;129;01min\u001b[39;00m tokenizers:\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/file_utils.py:2770\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2768\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: module transformers.models.bartpho has no attribute BartphoTokenizer"
     ]
    }
   ],
   "source": [
    "#BARTpho-syllable\n",
    "syllable_tokenizer = AutoTokenizer.from_pretrained(\"./\", use_fast=False)\n",
    "# sentencepiece.bpe.model\n",
    "# bartpho_syllable = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "# TXT = 'Chúng tôi là những nghiên cứu viên.'  \n",
    "# input_ids = syllable_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "# features = bartpho_syllable(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f2ce3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module transformers.models.bartpho has no attribute BartphoTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m syllable_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvinai/bartpho-syllable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:522\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     tokenizer_class_candidate \u001b[38;5;241m=\u001b[39m config_tokenizer_class\n\u001b[0;32m--> 522\u001b[0m     tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_class_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_class_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:253\u001b[0m, in \u001b[0;36mtokenizer_class_from_name\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m    250\u001b[0m         module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(module_name)\n\u001b[1;32m    252\u001b[0m         module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 253\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config, tokenizers \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tokenizer \u001b[38;5;129;01min\u001b[39;00m tokenizers:\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/file_utils.py:2770\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2768\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: module transformers.models.bartpho has no attribute BartphoTokenizer"
     ]
    }
   ],
   "source": [
    "syllable_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e433c7cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module transformers.models.bartpho has no attribute BartphoTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#BARTpho-syllable\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m syllable_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvinai/bartpho-syllable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m bartpho_syllable \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvinai/bartpho-syllable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m TXT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChúng tôi là những nghiên cứu viên.\u001b[39m\u001b[38;5;124m'\u001b[39m  \n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:522\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     tokenizer_class_candidate \u001b[38;5;241m=\u001b[39m config_tokenizer_class\n\u001b[0;32m--> 522\u001b[0m     tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_class_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_class_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:253\u001b[0m, in \u001b[0;36mtokenizer_class_from_name\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m    250\u001b[0m         module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(module_name)\n\u001b[1;32m    252\u001b[0m         module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 253\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config, tokenizers \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tokenizer \u001b[38;5;129;01min\u001b[39;00m tokenizers:\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/file_utils.py:2770\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2768\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: module transformers.models.bartpho has no attribute BartphoTokenizer"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "#BARTpho-syllable\n",
    "syllable_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\", use_fast=False)\n",
    "bartpho_syllable = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "TXT = 'Chúng tôi là những nghiên cứu viên.'  \n",
    "input_ids = syllable_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "features = bartpho_syllable(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe804fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Attention Is All You Need.pdf'\r\n",
      " BARTPho.ipynb\r\n",
      "'BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese.pdf'\r\n",
      " config.json\r\n",
      " dict.txt\r\n",
      " PhoMT.zip\r\n",
      " README.md\r\n",
      " references\r\n",
      " related\r\n",
      " sentencepiece.bpe.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85036317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9838b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.auto.tokenization_auto import get_tokenizer_config\n",
    "\n",
    "tokenizer.save_pretrained(\"tokenizer-test\")\n",
    "tokenizer_config = get_tokenizer_config(\"tokenizer-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc7a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_name plbart\n",
      "tokenizers (None, None)\n",
      "module_name fnet\n",
      "tokenizers ('FNetTokenizer', 'FNetTokenizerFast')\n",
      "module_name retribert\n",
      "tokenizers ('RetriBertTokenizer', 'RetriBertTokenizerFast')\n",
      "module_name roformer\n",
      "tokenizers ('RoFormerTokenizer', 'RoFormerTokenizerFast')\n",
      "module_name t5\n",
      "tokenizers (None, 'T5TokenizerFast')\n",
      "module_name mt5\n",
      "tokenizers (None, 'MT5TokenizerFast')\n",
      "module_name mobilebert\n",
      "tokenizers ('MobileBertTokenizer', 'MobileBertTokenizerFast')\n",
      "module_name distilbert\n",
      "tokenizers ('DistilBertTokenizer', 'DistilBertTokenizerFast')\n",
      "module_name albert\n",
      "tokenizers (None, 'AlbertTokenizerFast')\n",
      "module_name camembert\n",
      "tokenizers (None, 'CamembertTokenizerFast')\n",
      "module_name pegasus\n",
      "tokenizers (None, 'PegasusTokenizerFast')\n",
      "module_name mbart\n",
      "tokenizers (None, 'MBartTokenizerFast')\n",
      "module_name xlm-roberta\n",
      "tokenizers (None, 'XLMRobertaTokenizerFast')\n",
      "module_name marian\n",
      "tokenizers (None, None)\n",
      "module_name blenderbot-small\n",
      "tokenizers ('BlenderbotSmallTokenizer', None)\n",
      "module_name blenderbot\n",
      "tokenizers ('BlenderbotTokenizer', 'BlenderbotTokenizerFast')\n",
      "module_name bart\n",
      "tokenizers ('BartTokenizer', 'BartTokenizerFast')\n",
      "module_name longformer\n",
      "tokenizers ('LongformerTokenizer', 'LongformerTokenizerFast')\n",
      "module_name roberta\n",
      "tokenizers ('RobertaTokenizer', 'RobertaTokenizerFast')\n",
      "module_name reformer\n",
      "tokenizers (None, 'ReformerTokenizerFast')\n",
      "module_name electra\n",
      "tokenizers ('ElectraTokenizer', 'ElectraTokenizerFast')\n",
      "module_name funnel\n",
      "tokenizers ('FunnelTokenizer', 'FunnelTokenizerFast')\n",
      "module_name lxmert\n",
      "tokenizers ('LxmertTokenizer', 'LxmertTokenizerFast')\n",
      "module_name layoutlm\n",
      "tokenizers ('LayoutLMTokenizer', 'LayoutLMTokenizerFast')\n",
      "module_name layoutlmv2\n",
      "tokenizers ('LayoutLMv2Tokenizer', 'LayoutLMv2TokenizerFast')\n",
      "module_name layoutxlm\n",
      "tokenizers ('LayoutXLMTokenizer', 'LayoutXLMTokenizerFast')\n",
      "module_name dpr\n",
      "tokenizers ('DPRQuestionEncoderTokenizer', 'DPRQuestionEncoderTokenizerFast')\n",
      "module_name squeezebert\n",
      "tokenizers ('SqueezeBertTokenizer', 'SqueezeBertTokenizerFast')\n",
      "module_name bert\n",
      "tokenizers ('BertTokenizer', 'BertTokenizerFast')\n",
      "module_name openai-gpt\n",
      "tokenizers ('OpenAIGPTTokenizer', 'OpenAIGPTTokenizerFast')\n",
      "module_name gpt2\n",
      "tokenizers ('GPT2Tokenizer', 'GPT2TokenizerFast')\n",
      "module_name transfo-xl\n",
      "tokenizers ('TransfoXLTokenizer', None)\n",
      "module_name xlnet\n",
      "tokenizers (None, 'XLNetTokenizerFast')\n",
      "module_name flaubert\n",
      "tokenizers ('FlaubertTokenizer', None)\n",
      "module_name xlm\n",
      "tokenizers ('XLMTokenizer', None)\n",
      "module_name ctrl\n",
      "tokenizers ('CTRLTokenizer', None)\n",
      "module_name fsmt\n",
      "tokenizers ('FSMTTokenizer', None)\n",
      "module_name bert-generation\n",
      "tokenizers (None, None)\n",
      "module_name deberta\n",
      "tokenizers ('DebertaTokenizer', 'DebertaTokenizerFast')\n",
      "module_name deberta-v2\n",
      "tokenizers (None, None)\n",
      "module_name rag\n",
      "tokenizers ('RagTokenizer', None)\n",
      "module_name xlm-prophetnet\n",
      "tokenizers (None, None)\n",
      "module_name speech_to_text\n",
      "tokenizers (None, None)\n",
      "module_name speech_to_text_2\n",
      "tokenizers ('Speech2Text2Tokenizer', None)\n",
      "module_name m2m_100\n",
      "tokenizers (None, None)\n",
      "module_name prophetnet\n",
      "tokenizers ('ProphetNetTokenizer', None)\n",
      "module_name mpnet\n",
      "tokenizers ('MPNetTokenizer', 'MPNetTokenizerFast')\n",
      "module_name tapas\n",
      "tokenizers ('TapasTokenizer', None)\n",
      "module_name led\n",
      "tokenizers ('LEDTokenizer', 'LEDTokenizerFast')\n",
      "module_name convbert\n",
      "tokenizers ('ConvBertTokenizer', 'ConvBertTokenizerFast')\n",
      "module_name big_bird\n",
      "tokenizers (None, 'BigBirdTokenizerFast')\n",
      "module_name ibert\n",
      "tokenizers ('RobertaTokenizer', 'RobertaTokenizerFast')\n",
      "module_name qdqbert\n",
      "tokenizers ('BertTokenizer', 'BertTokenizerFast')\n",
      "module_name wav2vec2\n",
      "tokenizers ('Wav2Vec2CTCTokenizer', None)\n",
      "module_name hubert\n",
      "tokenizers ('Wav2Vec2CTCTokenizer', None)\n",
      "module_name gpt_neo\n",
      "tokenizers ('GPT2Tokenizer', 'GPT2TokenizerFast')\n",
      "module_name luke\n",
      "tokenizers ('LukeTokenizer', None)\n",
      "module_name mluke\n",
      "tokenizers (None, None)\n",
      "module_name bigbird_pegasus\n",
      "tokenizers ('PegasusTokenizer', 'PegasusTokenizerFast')\n",
      "module_name canine\n",
      "tokenizers ('CanineTokenizer', None)\n",
      "module_name bertweet\n",
      "tokenizers ('BertweetTokenizer', None)\n",
      "module_name bert-japanese\n",
      "tokenizers ('BertJapaneseTokenizer', None)\n",
      "module_name splinter\n",
      "tokenizers ('SplinterTokenizer', 'SplinterTokenizerFast')\n",
      "module_name byt5\n",
      "tokenizers ('ByT5Tokenizer', None)\n",
      "module_name cpm\n",
      "tokenizers (None, 'CpmTokenizerFast')\n",
      "module_name herbert\n",
      "tokenizers ('HerbertTokenizer', 'HerbertTokenizerFast')\n",
      "module_name phobert\n",
      "tokenizers ('PhobertTokenizer', None)\n",
      "module_name bartpho\n",
      "tokenizers ('BartphoTokenizer', None)\n",
      "module_name barthez\n",
      "tokenizers (None, 'BarthezTokenizerFast')\n",
      "module_name mbart50\n",
      "tokenizers (None, 'MBart50TokenizerFast')\n",
      "module_name rembert\n",
      "tokenizers (None, 'RemBertTokenizerFast')\n",
      "module_name clip\n",
      "tokenizers ('CLIPTokenizer', 'CLIPTokenizerFast')\n",
      "module_name wav2vec2_phoneme\n",
      "tokenizers ('Wav2Vec2PhonemeCTCTokenizer', None)\n",
      "module_name perceiver\n",
      "tokenizers ('PerceiverTokenizer', None)\n",
      "module_name xglm\n",
      "tokenizers (None, 'XGLMTokenizerFast')\n",
      "module_name plbart\n",
      "tokenizers (None, None)\n",
      "module_name fnet\n",
      "tokenizers ('FNetTokenizer', 'FNetTokenizerFast')\n",
      "module_name retribert\n",
      "tokenizers ('RetriBertTokenizer', 'RetriBertTokenizerFast')\n",
      "module_name roformer\n",
      "tokenizers ('RoFormerTokenizer', 'RoFormerTokenizerFast')\n",
      "module_name t5\n",
      "tokenizers (None, 'T5TokenizerFast')\n",
      "module_name mt5\n",
      "tokenizers (None, 'MT5TokenizerFast')\n",
      "module_name mobilebert\n",
      "tokenizers ('MobileBertTokenizer', 'MobileBertTokenizerFast')\n",
      "module_name distilbert\n",
      "tokenizers ('DistilBertTokenizer', 'DistilBertTokenizerFast')\n",
      "module_name albert\n",
      "tokenizers (None, 'AlbertTokenizerFast')\n",
      "module_name camembert\n",
      "tokenizers (None, 'CamembertTokenizerFast')\n",
      "module_name pegasus\n",
      "tokenizers (None, 'PegasusTokenizerFast')\n",
      "module_name mbart\n",
      "tokenizers (None, 'MBartTokenizerFast')\n",
      "module_name xlm-roberta\n",
      "tokenizers (None, 'XLMRobertaTokenizerFast')\n",
      "module_name marian\n",
      "tokenizers (None, None)\n",
      "module_name blenderbot-small\n",
      "tokenizers ('BlenderbotSmallTokenizer', None)\n",
      "module_name blenderbot\n",
      "tokenizers ('BlenderbotTokenizer', 'BlenderbotTokenizerFast')\n",
      "module_name bart\n",
      "tokenizers ('BartTokenizer', 'BartTokenizerFast')\n",
      "module_name longformer\n",
      "tokenizers ('LongformerTokenizer', 'LongformerTokenizerFast')\n",
      "module_name roberta\n",
      "tokenizers ('RobertaTokenizer', 'RobertaTokenizerFast')\n",
      "module_name reformer\n",
      "tokenizers (None, 'ReformerTokenizerFast')\n",
      "module_name electra\n",
      "tokenizers ('ElectraTokenizer', 'ElectraTokenizerFast')\n",
      "module_name funnel\n",
      "tokenizers ('FunnelTokenizer', 'FunnelTokenizerFast')\n",
      "module_name lxmert\n",
      "tokenizers ('LxmertTokenizer', 'LxmertTokenizerFast')\n",
      "module_name layoutlm\n",
      "tokenizers ('LayoutLMTokenizer', 'LayoutLMTokenizerFast')\n",
      "module_name layoutlmv2\n",
      "tokenizers ('LayoutLMv2Tokenizer', 'LayoutLMv2TokenizerFast')\n",
      "module_name layoutxlm\n",
      "tokenizers ('LayoutXLMTokenizer', 'LayoutXLMTokenizerFast')\n",
      "module_name dpr\n",
      "tokenizers ('DPRQuestionEncoderTokenizer', 'DPRQuestionEncoderTokenizerFast')\n",
      "module_name squeezebert\n",
      "tokenizers ('SqueezeBertTokenizer', 'SqueezeBertTokenizerFast')\n",
      "module_name bert\n",
      "tokenizers ('BertTokenizer', 'BertTokenizerFast')\n",
      "module_name openai-gpt\n",
      "tokenizers ('OpenAIGPTTokenizer', 'OpenAIGPTTokenizerFast')\n",
      "module_name gpt2\n",
      "tokenizers ('GPT2Tokenizer', 'GPT2TokenizerFast')\n",
      "module_name transfo-xl\n",
      "tokenizers ('TransfoXLTokenizer', None)\n",
      "module_name xlnet\n",
      "tokenizers (None, 'XLNetTokenizerFast')\n",
      "module_name flaubert\n",
      "tokenizers ('FlaubertTokenizer', None)\n",
      "module_name xlm\n",
      "tokenizers ('XLMTokenizer', None)\n",
      "module_name ctrl\n",
      "tokenizers ('CTRLTokenizer', None)\n",
      "module_name fsmt\n",
      "tokenizers ('FSMTTokenizer', None)\n",
      "module_name bert-generation\n",
      "tokenizers (None, None)\n",
      "module_name deberta\n",
      "tokenizers ('DebertaTokenizer', 'DebertaTokenizerFast')\n",
      "module_name deberta-v2\n",
      "tokenizers (None, None)\n",
      "module_name rag\n",
      "tokenizers ('RagTokenizer', None)\n",
      "module_name xlm-prophetnet\n",
      "tokenizers (None, None)\n",
      "module_name speech_to_text\n",
      "tokenizers (None, None)\n",
      "module_name speech_to_text_2\n",
      "tokenizers ('Speech2Text2Tokenizer', None)\n",
      "module_name m2m_100\n",
      "tokenizers (None, None)\n",
      "module_name prophetnet\n",
      "tokenizers ('ProphetNetTokenizer', None)\n",
      "module_name mpnet\n",
      "tokenizers ('MPNetTokenizer', 'MPNetTokenizerFast')\n",
      "module_name tapas\n",
      "tokenizers ('TapasTokenizer', None)\n",
      "module_name led\n",
      "tokenizers ('LEDTokenizer', 'LEDTokenizerFast')\n",
      "module_name convbert\n",
      "tokenizers ('ConvBertTokenizer', 'ConvBertTokenizerFast')\n",
      "module_name big_bird\n",
      "tokenizers (None, 'BigBirdTokenizerFast')\n",
      "module_name ibert\n",
      "tokenizers ('RobertaTokenizer', 'RobertaTokenizerFast')\n",
      "module_name qdqbert\n",
      "tokenizers ('BertTokenizer', 'BertTokenizerFast')\n",
      "module_name wav2vec2\n",
      "tokenizers ('Wav2Vec2CTCTokenizer', None)\n",
      "module_name hubert\n",
      "tokenizers ('Wav2Vec2CTCTokenizer', None)\n",
      "module_name gpt_neo\n",
      "tokenizers ('GPT2Tokenizer', 'GPT2TokenizerFast')\n",
      "module_name luke\n",
      "tokenizers ('LukeTokenizer', None)\n",
      "module_name mluke\n",
      "tokenizers (None, None)\n",
      "module_name bigbird_pegasus\n",
      "tokenizers ('PegasusTokenizer', 'PegasusTokenizerFast')\n",
      "module_name canine\n",
      "tokenizers ('CanineTokenizer', None)\n",
      "module_name bertweet\n",
      "tokenizers ('BertweetTokenizer', None)\n",
      "module_name bert-japanese\n",
      "tokenizers ('BertJapaneseTokenizer', None)\n",
      "module_name splinter\n",
      "tokenizers ('SplinterTokenizer', 'SplinterTokenizerFast')\n",
      "module_name byt5\n",
      "tokenizers ('ByT5Tokenizer', None)\n",
      "module_name cpm\n",
      "tokenizers (None, 'CpmTokenizerFast')\n",
      "module_name herbert\n",
      "tokenizers ('HerbertTokenizer', 'HerbertTokenizerFast')\n",
      "module_name phobert\n",
      "tokenizers ('PhobertTokenizer', None)\n",
      "module_name bartpho\n",
      "tokenizers ('BartphoTokenizer', None)\n",
      "module {'__name__': 'transformers.models.bartpho', '__doc__': None, '__package__': None, '__loader__': None, '__spec__': ModuleSpec(name='transformers.models.bartpho', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fddc21ded40>, origin='/home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/bartpho/__init__.py', submodule_search_locations=['/home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/bartpho']), '_modules': set(), '_class_to_module': {}, '__all__': [], '__file__': '/home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/bartpho/__init__.py', '__path__': ['/home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/bartpho'], '_objects': {}, '_name': 'transformers.models.bartpho', '_import_structure': {}}\n",
      "class_name BartphoTokenizer\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module transformers.models.bartpho has no attribute BartphoTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m bartpho \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvinai/bartpho-syllable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvinai/bartpho-syllable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:527\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     tokenizer_class_candidate \u001b[38;5;241m=\u001b[39m config_tokenizer_class\n\u001b[0;32m--> 527\u001b[0m     tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_class_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_class_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:258\u001b[0m, in \u001b[0;36mtokenizer_class_from_name\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;66;03m# print(\"module\", module)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_name)\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config, tokenizers \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tokenizer \u001b[38;5;129;01min\u001b[39;00m tokenizers:\n",
      "File \u001b[0;32m~/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/file_utils.py:2770\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2768\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: module transformers.models.bartpho has no attribute BartphoTokenizer"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "\n",
    "# line = \"Chúng tôi là những nghiên cứu viên.\"\n",
    "\n",
    "# input_ids = tokenizer(line, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     features = bartpho(**input_ids)  # Models outputs are now tuples\n",
    "\n",
    "# # With TensorFlow 2.0+:\n",
    "# from transformers import TFAutoModel\n",
    "\n",
    "# bartpho = TFAutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "# input_ids = tokenizer(line, return_tensors=\"tf\")\n",
    "# features = bartpho(**input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9132a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 21.2.4 from /home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/pip (python 3.10)\r\n"
     ]
    }
   ],
   "source": [
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de27dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3b75109d2f497899a4ef71cb261ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m bartpho \u001b[38;5;241m=\u001b[39m MBartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvinai/bartpho-syllable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m TXT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChúng tôi là <mask> nghiên cứu viên.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m([TXT], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m logits \u001b[38;5;241m=\u001b[39m bartpho(input_ids)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      7\u001b[0m masked_index \u001b[38;5;241m=\u001b[39m (input_ids[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "\n",
    "bartpho = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "TXT = \"Chúng tôi là <mask> nghiên cứu viên.\"\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = bartpho(input_ids).logits\n",
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "print(tokenizer.decode(predictions).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83371b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "bartpho_syllable = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "TXT = 'Chúng tôi là những nghiên cứu viên.'  \n",
    "input_ids = syllable_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "features = bartpho_syllable(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adec60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "bartpho_syllable = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "TXT = 'Chúng tôi là <mask> nghiên cứu viên.'\n",
    "input_ids = syllable_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "logits = bartpho_syllable(input_ids).logits\n",
    "masked_index = (input_ids[0] == syllable_tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "print(syllable_tokenizer.decode(predictions).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BARTpho-word\n",
    "word_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word\", use_fast=False)\n",
    "bartpho_word = AutoModel.from_pretrained(\"vinai/bartpho-word\")\n",
    "TXT = 'Chúng_tôi là những nghiên_cứu_viên .'  \n",
    "input_ids = word_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "features = bartpho_word(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5632f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bartpho_word = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-word\")\n",
    "TXT = 'Chúng_tôi là những <mask> .'\n",
    "input_ids = word_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "logits = bartpho_word(input_ids).logits\n",
    "masked_index = (input_ids[0] == word_tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "print(word_tokenizer.decode(predictions).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adc20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14268c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d7d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4dae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488e12f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
