{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6c6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "#BARTpho-syllable\n",
    "syllable_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\", use_fast=False)\n",
    "bartpho_syllable = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "TXT = 'Chúng tôi là những nghiên cứu viên.'  \n",
    "input_ids = syllable_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "features = bartpho_syllable(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016b5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fecf8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80078a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330822f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_name plbart\n",
      "tokenizers ('PLBartTokenizer', None)\n",
      "module_name fnet\n",
      "tokenizers ('FNetTokenizer', 'FNetTokenizerFast')\n",
      "module_name retribert\n",
      "tokenizers ('RetriBertTokenizer', 'RetriBertTokenizerFast')\n",
      "module_name roformer\n",
      "tokenizers ('RoFormerTokenizer', 'RoFormerTokenizerFast')\n",
      "module_name t5\n",
      "tokenizers ('T5Tokenizer', 'T5TokenizerFast')\n",
      "module_name mt5\n",
      "tokenizers ('MT5Tokenizer', 'MT5TokenizerFast')\n",
      "module_name mobilebert\n",
      "tokenizers ('MobileBertTokenizer', 'MobileBertTokenizerFast')\n",
      "module_name distilbert\n",
      "tokenizers ('DistilBertTokenizer', 'DistilBertTokenizerFast')\n",
      "module_name albert\n",
      "tokenizers ('AlbertTokenizer', 'AlbertTokenizerFast')\n",
      "module_name camembert\n",
      "tokenizers ('CamembertTokenizer', 'CamembertTokenizerFast')\n",
      "module_name pegasus\n",
      "tokenizers ('PegasusTokenizer', 'PegasusTokenizerFast')\n",
      "module_name mbart\n",
      "tokenizers ('MBartTokenizer', 'MBartTokenizerFast')\n",
      "module_name xlm-roberta\n",
      "tokenizers ('XLMRobertaTokenizer', 'XLMRobertaTokenizerFast')\n",
      "module_name marian\n",
      "tokenizers ('MarianTokenizer', None)\n",
      "module_name blenderbot-small\n",
      "tokenizers ('BlenderbotSmallTokenizer', None)\n",
      "module_name blenderbot\n",
      "tokenizers ('BlenderbotTokenizer', 'BlenderbotTokenizerFast')\n",
      "module_name bart\n",
      "tokenizers ('BartTokenizer', 'BartTokenizerFast')\n",
      "module_name longformer\n",
      "tokenizers ('LongformerTokenizer', 'LongformerTokenizerFast')\n",
      "module_name roberta\n",
      "tokenizers ('RobertaTokenizer', 'RobertaTokenizerFast')\n",
      "module_name reformer\n",
      "tokenizers ('ReformerTokenizer', 'ReformerTokenizerFast')\n",
      "module_name electra\n",
      "tokenizers ('ElectraTokenizer', 'ElectraTokenizerFast')\n",
      "module_name funnel\n",
      "tokenizers ('FunnelTokenizer', 'FunnelTokenizerFast')\n",
      "module_name lxmert\n",
      "tokenizers ('LxmertTokenizer', 'LxmertTokenizerFast')\n",
      "module_name layoutlm\n",
      "tokenizers ('LayoutLMTokenizer', 'LayoutLMTokenizerFast')\n",
      "module_name layoutlmv2\n",
      "tokenizers ('LayoutLMv2Tokenizer', 'LayoutLMv2TokenizerFast')\n",
      "module_name layoutxlm\n",
      "tokenizers ('LayoutXLMTokenizer', 'LayoutXLMTokenizerFast')\n",
      "module_name dpr\n",
      "tokenizers ('DPRQuestionEncoderTokenizer', 'DPRQuestionEncoderTokenizerFast')\n",
      "module_name squeezebert\n",
      "tokenizers ('SqueezeBertTokenizer', 'SqueezeBertTokenizerFast')\n",
      "module_name bert\n",
      "tokenizers ('BertTokenizer', 'BertTokenizerFast')\n",
      "module_name openai-gpt\n",
      "tokenizers ('OpenAIGPTTokenizer', 'OpenAIGPTTokenizerFast')\n",
      "module_name gpt2\n",
      "tokenizers ('GPT2Tokenizer', 'GPT2TokenizerFast')\n",
      "module_name transfo-xl\n",
      "tokenizers ('TransfoXLTokenizer', None)\n",
      "module_name xlnet\n",
      "tokenizers ('XLNetTokenizer', 'XLNetTokenizerFast')\n",
      "module_name flaubert\n",
      "tokenizers ('FlaubertTokenizer', None)\n",
      "module_name xlm\n",
      "tokenizers ('XLMTokenizer', None)\n",
      "module_name ctrl\n",
      "tokenizers ('CTRLTokenizer', None)\n",
      "module_name fsmt\n",
      "tokenizers ('FSMTTokenizer', None)\n",
      "module_name bert-generation\n",
      "tokenizers ('BertGenerationTokenizer', None)\n",
      "module_name deberta\n",
      "tokenizers ('DebertaTokenizer', 'DebertaTokenizerFast')\n",
      "module_name deberta-v2\n",
      "tokenizers ('DebertaV2Tokenizer', None)\n",
      "module_name rag\n",
      "tokenizers ('RagTokenizer', None)\n",
      "module_name xlm-prophetnet\n",
      "tokenizers ('XLMProphetNetTokenizer', None)\n",
      "module_name speech_to_text\n",
      "tokenizers ('Speech2TextTokenizer', None)\n",
      "module_name speech_to_text_2\n",
      "tokenizers ('Speech2Text2Tokenizer', None)\n",
      "module_name m2m_100\n",
      "tokenizers ('M2M100Tokenizer', None)\n",
      "module_name prophetnet\n",
      "tokenizers ('ProphetNetTokenizer', None)\n",
      "module_name mpnet\n",
      "tokenizers ('MPNetTokenizer', 'MPNetTokenizerFast')\n",
      "module_name tapas\n",
      "tokenizers ('TapasTokenizer', None)\n",
      "module_name led\n",
      "tokenizers ('LEDTokenizer', 'LEDTokenizerFast')\n",
      "module_name convbert\n",
      "tokenizers ('ConvBertTokenizer', 'ConvBertTokenizerFast')\n",
      "module_name big_bird\n",
      "tokenizers ('BigBirdTokenizer', 'BigBirdTokenizerFast')\n",
      "module_name ibert\n",
      "tokenizers ('RobertaTokenizer', 'RobertaTokenizerFast')\n",
      "module_name qdqbert\n",
      "tokenizers ('BertTokenizer', 'BertTokenizerFast')\n",
      "module_name wav2vec2\n",
      "tokenizers ('Wav2Vec2CTCTokenizer', None)\n",
      "module_name hubert\n",
      "tokenizers ('Wav2Vec2CTCTokenizer', None)\n",
      "module_name gpt_neo\n",
      "tokenizers ('GPT2Tokenizer', 'GPT2TokenizerFast')\n",
      "module_name luke\n",
      "tokenizers ('LukeTokenizer', None)\n",
      "module_name mluke\n",
      "tokenizers ('MLukeTokenizer', None)\n",
      "module_name bigbird_pegasus\n",
      "tokenizers ('PegasusTokenizer', 'PegasusTokenizerFast')\n",
      "module_name canine\n",
      "tokenizers ('CanineTokenizer', None)\n",
      "module_name bertweet\n",
      "tokenizers ('BertweetTokenizer', None)\n",
      "module_name bert-japanese\n",
      "tokenizers ('BertJapaneseTokenizer', None)\n",
      "module_name splinter\n",
      "tokenizers ('SplinterTokenizer', 'SplinterTokenizerFast')\n",
      "module_name byt5\n",
      "tokenizers ('ByT5Tokenizer', None)\n",
      "module_name cpm\n",
      "tokenizers ('CpmTokenizer', 'CpmTokenizerFast')\n",
      "module_name herbert\n",
      "tokenizers ('HerbertTokenizer', 'HerbertTokenizerFast')\n",
      "module_name phobert\n",
      "tokenizers ('PhobertTokenizer', None)\n",
      "module_name bartpho\n",
      "tokenizers ('BartphoTokenizer', None)\n",
      "module {'__name__': 'transformers.models.bartpho', '__doc__': None, '__package__': None, '__loader__': None, '__spec__': ModuleSpec(name='transformers.models.bartpho', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f44daa26860>, origin='/home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/bartpho/__init__.py', submodule_search_locations=['/home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/bartpho']), '_modules': {'tokenization_bartpho'}, '_class_to_module': {'BartphoTokenizer': 'tokenization_bartpho'}, '__all__': ['tokenization_bartpho', 'BartphoTokenizer'], '__file__': '/home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/bartpho/__init__.py', '__path__': ['/home/lap14784/miniconda3/envs/bartpho/lib/python3.10/site-packages/transformers/models/bartpho'], '_objects': {}, '_name': 'transformers.models.bartpho', '_import_structure': {'tokenization_bartpho': ['BartphoTokenizer']}}\n",
      "class_name BartphoTokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee77861c1b045ccae76987f3bf3ae4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9a1b5b7cac459bb21ca002711cc5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#BARTpho-syllable\n",
    "syllable_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\", use_fast=False)\n",
    "bartpho_syllable = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "TXT = 'Em có đồng <mask> hẹn hò với anh không ?'  \n",
    "input_ids = syllable_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "features = bartpho_syllable(input_ids)\n",
    "\n",
    "from transformers import MBartForConditionalGeneration\n",
    "bartpho_syllable = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "TXT = 'Em có đồng <mask> hẹn hò với anh không ?'\n",
    "input_ids = syllable_tokenizer(TXT, return_tensors='pt')['input_ids']\n",
    "logits = bartpho_syllable(input_ids).logits\n",
    "masked_index = (input_ids[0] == syllable_tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "print(syllable_tokenizer.decode(predictions).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffa5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
